# 2025广东省工科赛（只有任务一和任务二版）
在这次备战过程中，我们设计并制作了一台能够自主完成扫码、排爆（抓球）、打靶（反恐）和人质救援任务的机器人。从最初的迷茫到最终的丝滑运行，我们踩过了无数个坑，也积累了大量的实战经验。

这篇博客将从系统架构、视觉识别、运动控制、以及那些让我们头秃的调试细节，全方位复盘整个过程，希望能给同样在做机器人比赛的同学们提供一份保姆级的参考指南。

一、 系统架构：大脑与小脑的分工

我们采用了经典的 “上位机 + 下位机” 架构，这是处理复杂机器人任务的最优解。

1. 上位机（大脑）：树莓派 (Raspberry Pi)

职责：负责“看”和“想”。

核心任务：

读取并解析二维码（确定任务顺序）。

运行 OpenCV 视觉算法，识别红/绿/蓝球、靶心、以及不同形状的人质。

计算目标物体相对于画面中心的偏差 (X, Y)。

通过串口 (UART) 将简单的指令（如 D:120,50 或 L1）发送给下位机。

2. 下位机（小脑）：STM32F103/F407

职责：负责“动”。

核心任务：

接收树莓派的指令。

运动学解算：控制麦克纳姆轮底盘实现全向移动（前后左右、斜跑、自旋）。

闭环控制：利用电机编码器实现 PID 速度环/位置环，利用陀螺仪 (IMU) 保持航向稳定。

执行机构：控制 6 自由度机械臂进行抓取、放置；控制激光器开关。

二、 视觉系统：让机器人“看懂”世界

视觉是机器人的眼睛。我们在树莓派上使用 Python + OpenCV 实现了整套逻辑。

1. 核心挑战

光线干扰：赛场光线与实验室完全不同，导致颜色阈值失效。

目标丢失：车速过快导致目标瞬间移出视野。

多任务切换：既要找球（颜色），又要打靶（颜色+形状），还要救人（形状）。

2. 解决方案：V18 融合算法

我们将两套不同的视觉逻辑融合在了一个 merged_vision.py 脚本中：

双摄切换：

广角摄像头 (CAM_WIDE)：视野大，用于大范围搜索目标（返回 L/C/R 方位）。

下视摄像头 (CAM_DOWN)：视野小但精准，用于对齐抓取（返回精确的 X/Y 像素误差）。

抗干扰处理：

引入 medianBlur（中值滤波）去除椒盐噪声。

使用 min_area 面积过滤，忽略远处的背景色块。

动态阈值：在赛场现场，我们发现 HSV 的 S（饱和度）和 V（亮度）会随光线剧烈变化。我们通过现场采样（比如红色的 H=177, S=233, V=111），重新划定了宽容度更高的阈值范围。

关键代码片段（任务队列逻辑）：

# 收到 STM32 发来的二维码 "123"
if "qr_code" in cmd:
    task_queue = list("123") # 任务队列 ['1', '2', '3']

# 收到 "run_task:1"
elif "run_task" in cmd:
    target_val = task_queue[0] # 自动取出 '1' (红色)
    current_task = "APPROACH" # 进入广角寻找模式


三、 运动控制：从“抽搐”到“丝滑”的进化史

这是我们在调试中卡得最久、也是收获最大的部分。

1. 初版逻辑的崩溃（死锁之谜）

刚开始，我们的车子出现了诡异的现象：视觉数据一直在发，STM32 也能收到，但车子就是纹丝不动。
经过排查，发现是 “超时逻辑” 写错了。

错误写法：用循环计数器 count++ 来判断超时。因为 STM32 跑得太快，0.1秒就数完了几百次循环，导致程序认为“超时了”，在电机还没来得及动之前就强行停止了任务。

修复：改为 HAL_GetTick() 时间戳计时。无论 CPU 跑多快，3秒就是实打实的3秒，给足了电机启动时间。

2. 进阶问题：震荡与卡顿

解决了不动的问题后，车子开始“多动症”：在目标附近反复横跳，或者走两步停一步。

原因：

PID 增益太大，导致超调。

每一帧图像都发指令，指令堆积导致电机反应不过来。

静摩擦力导致小误差时推不动。

3. 终极方案：串行修正 + 脉冲驱动

我们最终设计了一套稳健的控制策略（详见 refined_mission_control.c）：

策略一：串行修正 (Sequential Adjustment)

先修 X 轴，再修 Y 轴。 绝不同时修。

麦克纳姆轮在斜着走时误差很大，分开修虽然慢一点点，但精度极高。

策略二：脉冲驱动 (Pulse Drive)

如果误差存在，给电机一个指令让它走。

然后强制“冷却” 0.8 秒。 在这期间不接受新指令，让车子完全走完、停稳。

再看一眼摄像头，还有误差？再来一个脉冲。

这种“动一下、停一下”的策略，完美解决了惯性导致的超调问题。

策略三：暴力起步 (Force Move)

如果算出来的移动距离只有 5mm（推不动车），强制给它加到 35mm。

宁可多走一点，也不能卡在原地不动。

四、 任务二（打靶）的路径规划

任务二需要在长距离移动（约 4米）后准确停在靶标前。这里单纯靠编码器（里程计）是不够的，因为地面打滑会导致走偏。

我们采用了 “分段 + 陀螺仪校正” 的跑法：

不一次性走完 1800mm。

而是拆分成：走 600mm -> 停下 -> 用陀螺仪检查角度 -> 原地旋转修正 -> 再走 600mm ...

代码实现：

case STATE_12_PART1_MOVE_A:     Chassis_Move_Forward(600.0f); break;
case STATE_12_PART1_CORRECT_A:  Turn_Angle(0.1); break; // 触发陀螺仪闭环


这种“步步为营”的策略，让我们的机器人在长距离直线上走得像尺子画出来一样直。

五、 机械臂动作调试

机械臂的动作需要与视觉流程紧密配合。

初始化：开机时复位到折叠状态，防止撞到场地设施。

抓取时机：视觉对齐完成后，不立即抓取，而是多等 0.5 秒。因为车子停下的瞬间会有惯性晃动，等稳住了再下爪，成功率 100%。

动作封装：我们将复杂的舵机控制封装成了 Arm_Start_Bomb_Grab 和 Arm_Start_Bomb_Place 函数，主逻辑里只需要调用函数名，代码清晰易读。

六、 总结与心得

不要相信软件的“快”：物理世界是有惯性和延迟的。代码里看似一瞬间完成的逻辑，电机可能需要几百毫秒才能响应。该加 Delay 的地方一定要加，该冷却的时候一定要冷却。

简单粗暴最有效：在复杂的赛场环境下，复杂的自适应算法往往不如一套简单的 “死区 + 最小步距 + 串行逻辑” 可靠。

数据说话：调试时不要猜。我们在 STM32 上加了大量的 printf，通过串口助手实时看 RX Raw（收到了什么）和 Action（执行了什么），这是解决问题的最快途径。
